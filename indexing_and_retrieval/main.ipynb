{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddd5146e",
   "metadata": {},
   "source": [
    "# Indexing and Retrieval — Activity 1 (Elasticsearch)\n",
    "\n",
    "This notebook preprocesses news + Wikipedia data, plots word frequency distributions (raw vs cleaned), indexes into Elasticsearch (`ESIndex-v1.0`), and measures latency/throughput and simple functional metrics.\n",
    "\n",
    "Prereqs: run `pip install -r requirements.txt`, and start Elasticsearch locally (e.g., Docker: `docker run -p 9200:9200 -e xpack.security.enabled=false docker.elastic.co/elasticsearch/elasticsearch:8.15.3`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90f4673d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas>=2.2.0 in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r ..\\requirements.txt (line 1)) (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r ..\\requirements.txt (line 2)) (2.3.4)\n",
      "Collecting matplotlib>=3.8.0 (from -r ..\\requirements.txt (line 3))\n",
      "  Downloading matplotlib-3.10.7-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: tqdm>=4.66.0 in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r ..\\requirements.txt (line 4)) (4.67.1)\n",
      "Collecting nltk>=3.8.1 (from -r ..\\requirements.txt (line 5))\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting regex>=2023.10.3 (from -r ..\\requirements.txt (line 6))\n",
      "  Downloading regex-2025.10.23-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: datasets>=3.0.0 in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r ..\\requirements.txt (line 7)) (4.2.0)\n",
      "Collecting elasticsearch>=8.14.0 (from -r ..\\requirements.txt (line 8))\n",
      "  Downloading elasticsearch-9.2.0-py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting elastic-transport>=8.13.0 (from -r ..\\requirements.txt (line 9))\n",
      "  Downloading elastic_transport-9.2.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting scipy>=1.11.0 (from -r ..\\requirements.txt (line 10))\n",
      "  Downloading scipy-1.16.3-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: psutil>=5.9.0 in c:\\users\\yashp\\appdata\\roaming\\python\\python313\\site-packages (from -r ..\\requirements.txt (line 11)) (7.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=2.2.0->-r ..\\requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=2.2.0->-r ..\\requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=2.2.0->-r ..\\requirements.txt (line 1)) (2025.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=3.8.0->-r ..\\requirements.txt (line 3))\n",
      "  Downloading contourpy-1.3.3-cp313-cp313-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib>=3.8.0->-r ..\\requirements.txt (line 3))\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=3.8.0->-r ..\\requirements.txt (line 3))\n",
      "  Downloading fonttools-4.60.1-cp313-cp313-win_amd64.whl.metadata (114 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib>=3.8.0->-r ..\\requirements.txt (line 3))\n",
      "  Downloading kiwisolver-1.4.9-cp313-cp313-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib>=3.8.0->-r ..\\requirements.txt (line 3)) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib>=3.8.0->-r ..\\requirements.txt (line 3))\n",
      "  Downloading pillow-12.0.0-cp313-cp313-win_amd64.whl.metadata (9.0 kB)\n",
      "Collecting pyparsing>=3 (from matplotlib>=3.8.0->-r ..\\requirements.txt (line 3))\n",
      "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>=4.66.0->-r ..\\requirements.txt (line 4)) (0.4.6)\n",
      "Collecting click (from nltk>=3.8.1->-r ..\\requirements.txt (line 5))\n",
      "  Downloading click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting joblib (from nltk>=3.8.1->-r ..\\requirements.txt (line 5))\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets>=3.0.0->-r ..\\requirements.txt (line 7)) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets>=3.0.0->-r ..\\requirements.txt (line 7)) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets>=3.0.0->-r ..\\requirements.txt (line 7)) (0.4.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets>=3.0.0->-r ..\\requirements.txt (line 7)) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets>=3.0.0->-r ..\\requirements.txt (line 7)) (0.28.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets>=3.0.0->-r ..\\requirements.txt (line 7)) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets>=3.0.0->-r ..\\requirements.txt (line 7)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->-r ..\\requirements.txt (line 7)) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets>=3.0.0->-r ..\\requirements.txt (line 7)) (0.35.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets>=3.0.0->-r ..\\requirements.txt (line 7)) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->-r ..\\requirements.txt (line 7)) (3.13.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0->datasets>=3.0.0->-r ..\\requirements.txt (line 7)) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0->datasets>=3.0.0->-r ..\\requirements.txt (line 7)) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0->datasets>=3.0.0->-r ..\\requirements.txt (line 7)) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0->datasets>=3.0.0->-r ..\\requirements.txt (line 7)) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=3.0.0->-r ..\\requirements.txt (line 7)) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets>=3.0.0->-r ..\\requirements.txt (line 7)) (4.15.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from elasticsearch>=8.14.0->-r ..\\requirements.txt (line 8)) (1.3.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from elastic-transport>=8.13.0->-r ..\\requirements.txt (line 9)) (2.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->-r ..\\requirements.txt (line 7)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->-r ..\\requirements.txt (line 7)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->-r ..\\requirements.txt (line 7)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->-r ..\\requirements.txt (line 7)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->-r ..\\requirements.txt (line 7)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->-r ..\\requirements.txt (line 7)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->-r ..\\requirements.txt (line 7)) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.2.0->-r ..\\requirements.txt (line 1)) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\yashp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.32.2->datasets>=3.0.0->-r ..\\requirements.txt (line 7)) (3.4.4)\n",
      "Downloading matplotlib-3.10.7-cp313-cp313-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.3/8.1 MB 6.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.9/8.1 MB 9.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.3/8.1 MB 10.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 10.2 MB/s  0:00:00\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 10.4 MB/s  0:00:00\n",
      "Downloading regex-2025.10.23-cp313-cp313-win_amd64.whl (276 kB)\n",
      "Downloading elasticsearch-9.2.0-py3-none-any.whl (960 kB)\n",
      "   ---------------------------------------- 0.0/960.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 960.5/960.5 kB 9.9 MB/s  0:00:00\n",
      "Downloading elastic_transport-9.2.0-py3-none-any.whl (65 kB)\n",
      "Downloading scipy-1.16.3-cp313-cp313-win_amd64.whl (38.5 MB)\n",
      "   ---------------------------------------- 0.0/38.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 2.4/38.5 MB 11.8 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 4.7/38.5 MB 11.8 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 7.1/38.5 MB 11.7 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 8.9/38.5 MB 11.2 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 10.5/38.5 MB 10.2 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 11.8/38.5 MB 9.5 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 13.1/38.5 MB 9.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 14.4/38.5 MB 8.7 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 15.7/38.5 MB 8.5 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 17.3/38.5 MB 8.3 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 18.6/38.5 MB 8.2 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 20.2/38.5 MB 8.1 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 21.5/38.5 MB 8.0 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 23.1/38.5 MB 8.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 24.9/38.5 MB 7.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 26.5/38.5 MB 7.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 28.0/38.5 MB 7.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 29.6/38.5 MB 7.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 31.2/38.5 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 32.8/38.5 MB 7.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 34.6/38.5 MB 7.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 36.4/38.5 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.0/38.5 MB 8.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.5/38.5 MB 7.9 MB/s  0:00:04\n",
      "Downloading contourpy-1.3.3-cp313-cp313-win_amd64.whl (226 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.60.1-cp313-cp313-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   -------------------------------- ------- 1.8/2.3 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 8.3 MB/s  0:00:00\n",
      "Downloading kiwisolver-1.4.9-cp313-cp313-win_amd64.whl (73 kB)\n",
      "Downloading pillow-12.0.0-cp313-cp313-win_amd64.whl (7.0 MB)\n",
      "   ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 1.8/7.0 MB 8.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 3.7/7.0 MB 8.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 5.2/7.0 MB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.0/7.0 MB 8.9 MB/s  0:00:00\n",
      "Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Downloading click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Installing collected packages: scipy, regex, pyparsing, pillow, kiwisolver, joblib, fonttools, elastic-transport, cycler, contourpy, click, nltk, matplotlib, elasticsearch\n",
      "\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----------------------------------------  0/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [pyparsing]\n",
      "   ----- ----------------------------------  2/14 [pyparsing]\n",
      "   -------- -------------------------------  3/14 [pillow]\n",
      "   -------- -------------------------------  3/14 [pillow]\n",
      "   -------- -------------------------------  3/14 [pillow]\n",
      "   -------- -------------------------------  3/14 [pillow]\n",
      "   -------- -------------------------------  3/14 [pillow]\n",
      "   -------------- -------------------------  5/14 [joblib]\n",
      "   -------------- -------------------------  5/14 [joblib]\n",
      "   -------------- -------------------------  5/14 [joblib]\n",
      "   ----------------- ----------------------  6/14 [fonttools]\n",
      "   ----------------- ----------------------  6/14 [fonttools]\n",
      "   ----------------- ----------------------  6/14 [fonttools]\n",
      "   ----------------- ----------------------  6/14 [fonttools]\n",
      "   ----------------- ----------------------  6/14 [fonttools]\n",
      "   ----------------- ----------------------  6/14 [fonttools]\n",
      "   ----------------- ----------------------  6/14 [fonttools]\n",
      "   ----------------- ----------------------  6/14 [fonttools]\n",
      "   ----------------- ----------------------  6/14 [fonttools]\n",
      "   ----------------- ----------------------  6/14 [fonttools]\n",
      "   ----------------- ----------------------  6/14 [fonttools]\n",
      "   ----------------- ----------------------  6/14 [fonttools]\n",
      "   -------------------- -------------------  7/14 [elastic-transport]\n",
      "   ---------------------------- ----------- 10/14 [click]\n",
      "   ------------------------------- -------- 11/14 [nltk]\n",
      "   ------------------------------- -------- 11/14 [nltk]\n",
      "   ------------------------------- -------- 11/14 [nltk]\n",
      "   ------------------------------- -------- 11/14 [nltk]\n",
      "   ------------------------------- -------- 11/14 [nltk]\n",
      "   ------------------------------- -------- 11/14 [nltk]\n",
      "   ------------------------------- -------- 11/14 [nltk]\n",
      "   ------------------------------- -------- 11/14 [nltk]\n",
      "   ------------------------------- -------- 11/14 [nltk]\n",
      "   ------------------------------- -------- 11/14 [nltk]\n",
      "   ------------------------------- -------- 11/14 [nltk]\n",
      "   ---------------------------------- ----- 12/14 [matplotlib]\n",
      "   ---------------------------------- ----- 12/14 [matplotlib]\n",
      "   ---------------------------------- ----- 12/14 [matplotlib]\n",
      "   ---------------------------------- ----- 12/14 [matplotlib]\n",
      "   ---------------------------------- ----- 12/14 [matplotlib]\n",
      "   ---------------------------------- ----- 12/14 [matplotlib]\n",
      "   ---------------------------------- ----- 12/14 [matplotlib]\n",
      "   ---------------------------------- ----- 12/14 [matplotlib]\n",
      "   ---------------------------------- ----- 12/14 [matplotlib]\n",
      "   ---------------------------------- ----- 12/14 [matplotlib]\n",
      "   ---------------------------------- ----- 12/14 [matplotlib]\n",
      "   ---------------------------------- ----- 12/14 [matplotlib]\n",
      "   ---------------------------------- ----- 12/14 [matplotlib]\n",
      "   ---------------------------------- ----- 12/14 [matplotlib]\n",
      "   ---------------------------------- ----- 12/14 [matplotlib]\n",
      "   ---------------------------------- ----- 12/14 [matplotlib]\n",
      "   ---------------------------------- ----- 12/14 [matplotlib]\n",
      "   ------------------------------------- -- 13/14 [elasticsearch]\n",
      "   ------------------------------------- -- 13/14 [elasticsearch]\n",
      "   ------------------------------------- -- 13/14 [elasticsearch]\n",
      "   ------------------------------------- -- 13/14 [elasticsearch]\n",
      "   ------------------------------------- -- 13/14 [elasticsearch]\n",
      "   ------------------------------------- -- 13/14 [elasticsearch]\n",
      "   ------------------------------------- -- 13/14 [elasticsearch]\n",
      "   ------------------------------------- -- 13/14 [elasticsearch]\n",
      "   ------------------------------------- -- 13/14 [elasticsearch]\n",
      "   ------------------------------------- -- 13/14 [elasticsearch]\n",
      "   ---------------------------------------- 14/14 [elasticsearch]\n",
      "\n",
      "Successfully installed click-8.3.0 contourpy-1.3.3 cycler-0.12.1 elastic-transport-9.2.0 elasticsearch-9.2.0 fonttools-4.60.1 joblib-1.5.2 kiwisolver-1.4.9 matplotlib-3.10.7 nltk-3.9.2 pillow-12.0.0 pyparsing-3.2.5 regex-2025.10.23 scipy-1.16.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "c:\\Users\\yashp\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ..\\requirements.txt\n",
    "\n",
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from preprocess import TextPreprocessor, PreprocessConfig\n",
    "from es_index import get_es, ensure_index, bulk_index\n",
    "from metrics import measure_latency, measure_throughput, percentile_latencies, precision_recall_at_k\n",
    "\n",
    "ES_INDEX = \"ESIndex-v1.0\"\n",
    "SAMPLE_SIZE_PER_SOURCE = 5000  # Adjust if you need a smaller/faster run\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7880bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data:   0%|          | 0/41 [00:00<?, ?files/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Downloading data:   2%|▏         | 1/41 [00:18<12:20, 18.52s/files]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Downloading data:   5%|▍         | 2/41 [00:50<17:07, 26.35s/files]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Downloading data:   7%|▋         | 3/41 [01:19<17:25, 27.51s/files]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Downloading data:  10%|▉         | 4/41 [01:49<17:36, 28.55s/files]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Downloading data:  12%|█▏        | 5/41 [02:16<16:51, 28.11s/files]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Downloading data:  15%|█▍        | 6/41 [02:38<15:09, 25.98s/files]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "def load_webz_io_sample(n: int) -> pd.DataFrame:\n",
    "    # Webz.io sample via webhose free news datasets often come as CSV/JSON lines.\n",
    "    # For simplicity we’ll use a small remote subset via datasets if available; otherwise expect a local file path set.\n",
    "    # Fallback: empty DataFrame.\n",
    "    try:\n",
    "        ds = load_dataset(\"webhose/news-category-dataset\")  # may not exist; replace with actual if available\n",
    "        df = ds[\"train\"].to_pandas()\n",
    "        df = df.head(n)\n",
    "        df = df.rename(columns={\"headline\": \"title\", \"short_description\": \"text\"})\n",
    "        df[\"source\"] = \"webz\"\n",
    "        df[\"doc_id\"] = df.index.map(lambda i: f\"webz-{i}\")\n",
    "        return df[[\"doc_id\", \"title\", \"text\", \"source\"]].dropna()\n",
    "    except Exception:\n",
    "        return pd.DataFrame(columns=[\"doc_id\", \"title\", \"text\", \"source\"])  # placeholder\n",
    "\n",
    "\n",
    "def load_wikipedia_en_sample(n: int) -> pd.DataFrame:\n",
    "    # HuggingFace: wikimedia/wikipedia split 20231101.en\n",
    "    ds = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split=\"train\")\n",
    "    # Columns: e.g., id, url, title, text\n",
    "    df = ds.to_pandas().head(n)\n",
    "    df[\"source\"] = \"wiki\"\n",
    "    df[\"doc_id\"] = df[\"id\"].apply(lambda x: f\"wiki-{x}\")\n",
    "    df[\"title\"] = df.get(\"title\", \"\")\n",
    "    df[\"text\"] = df.get(\"text\", \"\")\n",
    "    return df[[\"doc_id\", \"title\", \"text\", \"source\"]].dropna()\n",
    "\n",
    "\n",
    "webz_df = load_webz_io_sample(SAMPLE_SIZE_PER_SOURCE)\n",
    "wikip_df = load_wikipedia_en_sample(SAMPLE_SIZE_PER_SOURCE)\n",
    "\n",
    "data_df = pd.concat([webz_df, wikip_df], ignore_index=True)\n",
    "print(f\"Loaded documents: {len(data_df)} (webz={len(webz_df)}, wiki={len(wikip_df)})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a0dfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word frequency plots (raw vs preprocessed)\n",
    "raw_texts = (data_df[\"title\"].fillna(\"\") + \"\\n\" + data_df[\"text\"].fillna(\"\")).tolist()\n",
    "\n",
    "pp_raw = TextPreprocessor(PreprocessConfig(lowercase=False, remove_stopwords=False, stem=False))\n",
    "pp_clean = TextPreprocessor(PreprocessConfig(lowercase=True, remove_stopwords=True, stem=True))\n",
    "\n",
    "from collections import Counter\n",
    "raw_counter = Counter()\n",
    "clean_counter = Counter()\n",
    "\n",
    "SAMPLE_FOR_PLOTS = min(10000, len(raw_texts))\n",
    "for t in raw_texts[:SAMPLE_FOR_PLOTS]:\n",
    "    raw_counter.update(pp_raw.tokenize(t))\n",
    "    clean_counter.update(pp_clean.tokenize(t))\n",
    "\n",
    "raw_top = raw_counter.most_common(30)\n",
    "clean_top = clean_counter.most_common(30)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "axes[0].bar([w for w,_ in raw_top], [c for _,c in raw_top])\n",
    "axes[0].set_title(\"Top 30 words (raw)\")\n",
    "axes[0].tick_params(axis='x', rotation=90)\n",
    "\n",
    "axes[1].bar([w for w,_ in clean_top], [c for _,c in clean_top])\n",
    "axes[1].set_title(\"Top 30 words (preprocessed)\")\n",
    "axes[1].tick_params(axis='x', rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd9c1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index into Elasticsearch\n",
    "es = get_es()\n",
    "ensure_index(es, ES_INDEX)\n",
    "\n",
    "records = data_df.to_dict(orient=\"records\")\n",
    "success_count, failed_count = bulk_index(es, ES_INDEX, records, batch_size=1000)\n",
    "print({\"indexed\": success_count, \"failed\": failed_count})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fc3ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple search wrapper for metrics\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "def es_search_fn(q: str):\n",
    "    body = {\n",
    "        \"query\": {\n",
    "            \"query_string\": {\n",
    "                \"query\": q,\n",
    "                \"fields\": [\"title^2\", \"text\"],\n",
    "                \"default_operator\": \"AND\"\n",
    "            }\n",
    "        },\n",
    "        \"size\": 20\n",
    "    }\n",
    "    resp = es.search(index=ES_INDEX, body=body)\n",
    "    hits = resp.get(\"hits\", {}).get(\"hits\", [])\n",
    "    return [h.get(\"_id\") for h in hits]\n",
    "\n",
    "# Create a small diverse query set (replace with LLM-probed set and justification in report)\n",
    "query_set = [\n",
    "    '\"climate change\" AND policy',\n",
    "    '\"football\" AND (world OR cup)',\n",
    "    '\"quantum computing\" AND algorithms',\n",
    "    'NOT \"covid\" AND vaccination',\n",
    "    '(\"space exploration\" AND mars) OR mission',\n",
    "]\n",
    "\n",
    "# Latency and throughput\n",
    "latencies, perc = measure_latency(es_search_fn, query_set)\n",
    "qps = measure_throughput(es_search_fn, query_set * 5)\n",
    "print({\"latency_ms\": perc, \"throughput_qps\": qps})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09a65d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional metrics (placeholder) — requires relevance judgments\n",
    "# Provide a relevance list per query (doc_id list). Replace with your judged results.\n",
    "relevance_judgments = {\n",
    "    \"climate change policy\": [],\n",
    "    \"football world cup\": [],\n",
    "    \"quantum computing algorithms\": [],\n",
    "    \"covid vaccination effectiveness\": [],\n",
    "    \"space exploration mars mission\": [],\n",
    "}\n",
    "\n",
    "precisions = {}\n",
    "recalls = {}\n",
    "for q in query_set:\n",
    "    predicted = es_search_fn(q)\n",
    "    relevant = relevance_judgments.get(q, [])\n",
    "    p, r = precision_recall_at_k(predicted, relevant, k=10)\n",
    "    precisions[q] = p\n",
    "    recalls[q] = r\n",
    "\n",
    "print({\"precision@10\": precisions, \"recall@10\": recalls})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42979f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory footprint (process RSS)\n",
    "import psutil, os\n",
    "process = psutil.Process(os.getpid())\n",
    "rss_mb = process.memory_info().rss / (1024*1024)\n",
    "print({\"process_rss_mb\": round(rss_mb, 2)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2105fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and query SelfIndex variants\n",
    "from self_index import SelfIndex\n",
    "\n",
    "# Variant controls: x (info), y (datastore), z (compression), i (optim), q (qproc)\n",
    "# x: BOOLEAN|WORDCOUNT|TFIDF  -> 1|2|3\n",
    "# y: CUSTOM -> 1 (only y=1 implemented here)\n",
    "# z: NONE|CODE|CLIB -> 0|1|2 (we'll use CODE and CLIB)\n",
    "# i: Null|Skipping -> 0|1 (skipping present internally but not toggled here)\n",
    "# q: TERMatat|DOCatat -> T|D\n",
    "\n",
    "variant = dict(info=\"TFIDF\", dstore=\"CUSTOM\", compr=\"CODE\", qproc=\"TERMatat\", optim=\"Null\")\n",
    "idx = SelfIndex(core='SelfIndex', info=variant['info'], dstore=variant['dstore'], qproc=variant['qproc'], compr=variant['compr'], optim=variant['optim'])\n",
    "index_id = f\"{idx.identifier_short}\"\n",
    "\n",
    "# Prepare files iterable from dataframe\n",
    "files_iter = [(r['doc_id'], (str(r['title']) + \"\\n\" + str(r['text']))) for _, r in data_df.iterrows()]\n",
    "\n",
    "idx.create_index(index_id, files_iter)\n",
    "idx.load_index(f\"indices/{index_id}\")\n",
    "\n",
    "import json as _json\n",
    "\n",
    "def self_search_fn(q: str):\n",
    "    res = _json.loads(idx.query(q))\n",
    "    return [r['doc_id'] for r in res.get('results', [])]\n",
    "\n",
    "# Compare latency/throughput with ES for the same query set\n",
    "latencies_self, perc_self = measure_latency(self_search_fn, query_set)\n",
    "qps_self = measure_throughput(self_search_fn, query_set * 5)\n",
    "print({\"self_latency_ms\": perc_self, \"self_throughput_qps\": qps_self})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0a670d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
